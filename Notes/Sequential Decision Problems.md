## Environment & Space
____
![[Figure 17.1.png]]
Beginning in the start state, you choose an action at each time step. The environment terminates when the agent reaches one of the goal states, marked as +1 or -1. Actions available are in $ACTIONS(s)$ or $A(s)$. The actions in every state are *Up, Down, Left,* and *Right*. This environment is **fully observable**. If the environment was **deterministic**, an easy solution would be $[Up, Up, Right, Right, Right]$. Because the environment is stochastic where every action has a probability, the solution would reach the goal with a probability of $0.8^5$. Figure 17.1 says that the intended action occurs with a probability of 0.8. 
## Uncertainty - Stochastic
____
The outcome is **stochastic**, so we write $P(s'|s,a)$ to denote the probability of reaching state $s'$ if the actions $a$ is done with the state $s$. The transitions are **Markovian**. **Transition model** describe the outcome of each action in each state. Within a stochastic environment, the transition models can be represented as **dynamic Bayesian Networks**. When there is no uncertainty (**deterministic**) an action causes a transition from a state to another. When there is uncertainty (**stochastic**), the action $a$ causes a transition from state $s$ to $s'$ with some probability.
## Utility Function
____
**Utility Function** will depend on a sequence of states called the **environment history**. In each state $s$, the agent will receive a **reward** $R(s)$, can be *positive* or *negative*, but must be *bounded*. The utility of an environment history is the sum of the rewards received. 

The reward in each state except the terminal is $-0.04$ for **Figure 17.1**. For example, the agent reaches the +1 state after 10 steps, its total utility will be $0.6$. The negative reward of $-0.04$ gives the agent an incentive to reach terminal quickly. 

## Markov Decision Process
____
A sequential decision problem for a **fully observable**, **stochastic** environment with a Markovian transition model and additive rewards is called **MDP**, or **Markov Decision Process**, and consists of set of states (initial state called $s_0$); a set $ACTIONS(s)$ of actions in each state; a transition model $P(s'|s,a)$; and a reward function $R(s)$. 

### Policy
____
A solution must specify what the agent should do for any state that the agent might reach. A solution of this kind is called a **policy**. This is denoted as $\pi$ and $\pi (s)$ is the solution recommended by the policy $\pi$ for the state $s$. If the agent has a complete policy, then no matter what the outcome of any action, the agent will know what to do next. 

A **proper policy** is guaranteed to reach a terminal state. With proper policies, we use additive rewards. For example, the first three policies in Figure 17.2 is proper but the fourth is improper. It gains infinite total rewards by staying away from the terminal states. The existence of **improper policies** cause the need for solving MDP to fail with additive rewards so therefore its a good reason to use **discounted rewards** with a discount factor. 

Infinite sequence can be compared using the **average reward** obtained per time step. Suppose that square (1,1) in the 4 × 3 world has a reward of 0.1 while the other nonterminal states have a reward of 0.01. Then a policy that does its best to stay in (1,1) will have higher average reward than one that stays elsewhere. 
#### Optimal Policy
____
The quality of a policy is therefore measured by the *expected* utility of the possible environment histories generated by that policy. An **optimal policy** is a policy that yields the highest expected utility. We denote this with $\pi ^*$ Given $\pi ^*$, the agent decides what to do by its current percept, which tells it the current state $s$, and then executing the action $\pi ^*(s)$. 

![[Optimal Policy Maze Example.png | 500]]
![[Stochastic State.png]]

### Importance of Reward
_____
![[Figure 17.2.png]]
The optimal path depends on the reward value in each nonterminal states. When the reward is positive in the nonterminal states, the agent seeks not to terminate. 


### Horizons
___
A finite horizon means that there is a fixed time N after which nothing matters, the game is over. Given a finite horizon, the optimal action in a given state could change over the given time N. The optimal policy for a finite horizon is **nonstationary**. If there is no fixed time limit, the optimal action depends only on the current state, and the policy is **stationary**. 

When a policy is stationary, you assign utilities using: 
**Additive Rewards**: 
- The utility of a state sequence is
$$U_h([s_0,s_1,s_2,...])=R(s_0)+R(s_1)+R(s_2)_...$$
- Figure 17.1 uses additive rewards. 

**Discounted Rewards**:
- The utility of a state sequence is
$$U_h([s_0,s_1,s_2,...])=R(s_0)+ \gamma R(s_1)+ \gamma ^2R(s_2)_...$$
- The **discount factor** $\gamma$ is the number between 0 to 1. This describes the preference of an agent for current rewards over future rewards. When the discount factor is close to 0, rewards in the distance future are viewed as insignificant. When it's close to 1, discount rewards are exactly equivalent to additive rewards. 
- With discounted rewards the utility of an infinite sequence is finite. In fact is the discount factor is less than 1, $\gamma < 1$ and the rewards are bounded $-R_{max}, +R_{max}$, we have $$U_h([s_0,s_1,s_2,...])= \sum_{t=0}^{\infty}\gamma ^t R(s_t)\le \sum_{t=0}^{\infty}\gamma ^t R_{max} = R_{max} / (1-\gamma)$$
### Utility of States
____
Utility of a given state sequence is the sum of discounted rewards during the sequence. We can compare policies by comparing the expected utilities obtained when executing them. $s$ is the initial state and defined $S_t$ to be the state the agent reaches at time $t$ when executing a particular policy $\pi$. The probability distribution over state sequence $S_1,S_2,...$ is determined by the initial state $s$ and the policy $\pi$ and the transition model for the environment.
$$U^{\pi}(s)=E[\sum_{t=0}^{\infty}\gamma ^t R(S_t)]]$$
The policies the agent chooses starting at $s$, one or more will have higher excepted utilities than others. We use $\pi ^*_s$ to denote one of these policies.
$$\pi ^*_s = argmaxU^{\pi} (s)$$![[Figure 17.3.png]]
The utilities is shown for the this environment. Utilities are higher for states closer to the $+1$ exit because fewer steps are needed to reach the terminal state.
$π^∗(s) = argmax \sum_{s'}P(s′ |s,a)U(s′)$ 

